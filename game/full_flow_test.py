#!/usr/bin/env python3
"""
NagaAgent Game å…¨æµç¨‹åŠ¨æ€æµ‹è¯•

å®Œå…¨æ— æšä¸¾çš„ç«¯åˆ°ç«¯æµ‹è¯•ï¼š
1. æ¥å—ç”¨æˆ·çœŸå®é—®é¢˜
2. åŠ¨æ€æ¨ç†é¢†åŸŸ
3. åŠ¨æ€ç”Ÿæˆæ™ºèƒ½ä½“å›¢é˜Ÿ
4. æ‰§è¡Œå®Œæ•´åšå¼ˆæµç¨‹
5. è¾“å‡ºè¯¦ç»†æ—¥å¿—å’Œç»“æœæ–‡æ¡£
"""

import asyncio
import json
import os
import sys
import time
from pathlib import Path
from typing import Any, Dict, List

# å…è®¸ä» game å­ç›®å½•ç›´æ¥æ‰§è¡Œæœ¬è„šæœ¬æ—¶ï¼Œæ­£ç¡®å¯¼å…¥é¡¶å±‚åŒ… `game`
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = Path(CURRENT_DIR).parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥ç³»ç»Ÿç»„ä»¶
from full_flow_logger import FullFlowTestLogger
from game.naga_game_system import NagaGameSystem
from game.core.models.config import GameConfig
from game.core.models.data_models import Task
from game.core.self_game.game_engine import GameEngine

class MockNagaConversation:
    """æ¨¡æ‹ŸNagaConversationï¼ˆå·²å¼ƒç”¨ï¼‰"""
    pass

async def run_full_flow_test(user_question: str):
    """è¿è¡Œå®Œæ•´æµç¨‹æµ‹è¯•"""
    
    # åˆ›å»ºæµ‹è¯•æ—¥å¿—å™¨
    test_logger = FullFlowTestLogger("åŠ¨æ€å…¨æµç¨‹æµ‹è¯•")
    test_logger.log_step("æµ‹è¯•å¼€å§‹", {"question": user_question})
    test_logger.test_data["user_question"] = user_question
    
    try:
        # 1. åˆå§‹åŒ–ç³»ç»Ÿ
        test_logger.log_step("ç³»ç»Ÿåˆå§‹åŒ–")
        
        # è¯»å–é…ç½®
        config_path = Path("../config.json")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            test_logger.log_step("é…ç½®åŠ è½½æˆåŠŸ", {"api_model": config_data.get("api", {}).get("model")})
        else:
            config_data = {}
            test_logger.log_step("ä½¿ç”¨é»˜è®¤é…ç½®")
        
        # åˆ›å»ºæ¸¸æˆç³»ç»Ÿï¼ˆä½¿ç”¨çœŸå® NagaConversationï¼Œç”±ç³»ç»Ÿå†…è‡ªåŠ¨åˆå§‹åŒ–ï¼‰
        game_config = GameConfig()
        naga_system = NagaGameSystem(game_config)
        
        test_logger.log_step("NagaGameSystemåˆ›å»ºæˆåŠŸ")
        
        # 2. é¢†åŸŸæ¨æ–­
        test_logger.log_step("å¼€å§‹é¢†åŸŸæ¨æ–­")
        inferred_domain = await naga_system._infer_domain_from_question(user_question)
        test_logger.test_data["inferred_domain"] = inferred_domain
        test_logger.log_node_output("é¢†åŸŸæ¨æ–­å™¨", "æ¨ç†", f"æ¨æ–­é¢†åŸŸ: {inferred_domain}")
        
        # 3. åˆ›å»ºä»»åŠ¡
        test_logger.log_step("åˆ›å»ºä»»åŠ¡å¯¹è±¡")
        task = Task(
            task_id=f"test_{int(time.time())}",
            description=user_question,
            domain=inferred_domain,
            requirements=["æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "æä¾›ä¸“ä¸šè§£å†³æ–¹æ¡ˆ"],
            constraints=["æ—¶é—´æ•ˆç‡", "è´¨é‡ä¿è¯"]
        )
        
        # 4. ç”Ÿæˆæ™ºèƒ½ä½“å›¢é˜Ÿ
        test_logger.log_step("å¼€å§‹ç”Ÿæˆæ™ºèƒ½ä½“å›¢é˜Ÿ")
        agents = await naga_system.generate_agents_only(task, (3, 5))

        if naga_system.role_generator.last_prompt_export_path:
            prompt_path = str(naga_system.role_generator.last_prompt_export_path)
            test_logger.test_data["prompt_export_file"] = prompt_path
            test_logger.log_step("æç¤ºè¯å·²å¯¼å‡º", {"file": prompt_path})

        # è®°å½•ç”Ÿæˆçš„æ™ºèƒ½ä½“
        test_logger.test_data["generated_agents"] = []
        for agent in agents:
            agent_data = {
                "name": agent.name,
                "role": agent.role,
                "is_requester": agent.is_requester,
                "responsibilities": agent.responsibilities,
                "skills": agent.skills,
                "connection_permissions": agent.connection_permissions
            }
            test_logger.test_data["generated_agents"].append(agent_data)
            
            test_logger.log_node_output(
                agent.name, 
                "éœ€æ±‚æ–¹" if agent.is_requester else "æ‰§è¡Œè€…",
                f"è§’è‰²: {agent.role}, èŒè´£: {', '.join(agent.responsibilities[:3])}"
            )
        
        # 5. æ„å»ºäº¤äº’å›¾
        test_logger.log_step("æ„å»ºäº¤äº’å›¾")
        interaction_graph = await naga_system._execute_interaction_graph_phase(agents, task)
        test_logger.test_data["interaction_graph"] = {
            "agent_count": len(interaction_graph.agents),
            "connections": len([conn for agent in interaction_graph.agents for conn in agent.connection_permissions])
        }
        
        # 6. æ‰§è¡Œç”¨æˆ·é—®é¢˜å¤„ç†
        test_logger.log_step("æ‰§è¡Œç”¨æˆ·é—®é¢˜å¤„ç†æµç¨‹")
        response = await naga_system.user_interaction_handler.process_user_request(
            user_question, interaction_graph, "test_user"
        )
        
        test_logger.test_data["final_result"] = response.content
        test_logger.log_node_output("ç³»ç»Ÿ", "æœ€ç»ˆå“åº”", response.content)
        
        # 7. å¯åŠ¨è‡ªåšå¼ˆå¼•æ“ï¼ˆçœŸå®æ‰§è¡Œï¼Œä¸ä½¿ç”¨å ä½/æ¨¡æ‹Ÿï¼‰
        test_logger.log_step("å¯åŠ¨è‡ªåšå¼ˆå¼•æ“")
        engine = GameEngine(game_config)
        session = await engine.start_game_session(task, agents, context=None)

        # è®°å½•æ¯è½®çœŸå®è¾“å‡º
        id_to_name = {a.agent_id: a.name for a in agents}
        real_rounds: List[Dict[str, Any]] = []
        for rnd in session.rounds:
            rd = {
                "round": rnd.round_number,
                "phase": rnd.phase,
                "decision": rnd.decision,
                "round_time": rnd.round_time,
                "average_critical_score": rnd.metadata.get('average_critical_score'),
                "average_novelty_score": rnd.metadata.get('average_novelty_score'),
                "average_satisfaction_score": rnd.metadata.get('average_satisfaction_score'),
                "actors": [],
                "critics": [],
                "philoss": []
            }
            for ao in rnd.actor_outputs:
                test_logger.log_node_output(id_to_name.get(ao.agent_id, ao.agent_id), "ç”Ÿæˆ", ao.content)
                rd["actors"].append({
                    "agent": id_to_name.get(ao.agent_id, ao.agent_id),
                    "iteration": ao.iteration,
                    "time": ao.generation_time,
                    "len": len(ao.content)
                })
            for co in rnd.critic_outputs:
                # ä½¿ç”¨ summary_critique ä½œä¸ºæ—¥å¿—å±•ç¤º
                test_logger.log_node_output(id_to_name.get(co.critic_agent_id, co.critic_agent_id), "æ‰¹åˆ¤", co.summary_critique)
                rd["critics"].append({
                    "critic": id_to_name.get(co.critic_agent_id, co.critic_agent_id),
                    "overall_score": co.overall_score,
                    "satisfaction_score": co.satisfaction_score,
                    "target": co.target_output_id
                })
            for po in rnd.philoss_outputs:
                test_logger.log_node_output("PhilossChecker", "è¯„ä¼°", f"æ–°é¢–åº¦: {po.novelty_score:.3f}")
                rd["philoss"].append({
                    "target": po.target_content_id,
                    "novelty_score": po.novelty_score
                })
            real_rounds.append(rd)
        test_logger.test_data["game_rounds"] = real_rounds

        # è®°å½•å¸•ç´¯æ‰˜å‰æ²¿ï¼ˆæ¥è‡ªæœ€åä¸€è½® metadataï¼‰
        if session.rounds:
            last_meta = session.rounds[-1].metadata or {}
            test_logger.test_data["pareto_front"] = last_meta.get("pareto_front", [])
  
         
        # æµ‹è¯•æˆåŠŸ
        test_logger.finalize_test(True)
        
        return {
            "success": True,
            "result": (session.final_result.actor_output.content if (session.final_result and session.final_result.actor_output) else response.content),
            "agents_generated": len(agents),
            "domain": inferred_domain,
            "log_file": str(test_logger.log_file),
            "report_file": str(test_logger.report_file)
        }
    except Exception as e:
        test_logger.log_error(e, "å…¨æµç¨‹æ‰§è¡Œ")
        test_logger.finalize_test(False)
        
        return {
            "success": False,
            "error": str(e),
            "log_file": str(test_logger.log_file),
            "report_file": str(test_logger.report_file)
        }

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ® NagaAgent Game å…¨æµç¨‹åŠ¨æ€æµ‹è¯•")
    print("ğŸš« ä¸¥æ ¼éµå¾ªæ— æšä¸¾åŸåˆ™ï¼Œå®Œå…¨åŸºäºç”¨æˆ·è¾“å…¥åŠ¨æ€æ¨ç†")
    print("=" * 80)
    
    # è·å–ç”¨æˆ·çœŸå®é—®é¢˜
    user_question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆå°†åŸºäºæ­¤é—®é¢˜è¿›è¡Œå®Œæ•´æµç¨‹æµ‹è¯•ï¼‰: ").strip()
    
    if not user_question:
        print("âŒ æœªè¾“å…¥é—®é¢˜ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•é—®é¢˜")
        user_question = "æˆ‘æƒ³åˆ›å»ºä¸€ä¸ªå¸®åŠ©å­¦ç”Ÿå­¦ä¹ ç¼–ç¨‹çš„æ™ºèƒ½å¹³å°"
    
    print(f"ğŸ“ æµ‹è¯•é—®é¢˜: {user_question}")
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå…¨æµç¨‹æµ‹è¯•...\n")
    
    # æ‰§è¡Œæµ‹è¯•
    result = await run_full_flow_test(user_question)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 80)
    
    if result["success"]:
        print("âœ… å…¨æµç¨‹æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ¤– ç”Ÿæˆæ™ºèƒ½ä½“æ•°é‡: {result['agents_generated']}")
        print(f"ğŸ¯ æ¨æ–­é¢†åŸŸ: {result['domain']}")
        print(f"ğŸ“„ è¯¦ç»†æ—¥å¿—: {result['log_file']}")
        print(f"ğŸ“‹ æµ‹è¯•æŠ¥å‘Š: {result['report_file']}")
        
        print(f"\nğŸ’¬ æœ€ç»ˆå“åº”é¢„è§ˆ:")
        print("```")
        print(result["result"][:500] + ("..." if len(result["result"]) > 500 else ""))
        print("```")
        
    else:
        print("âŒ å…¨æµç¨‹æµ‹è¯•å¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {result['error']}")
        print(f"ğŸ“„ é”™è¯¯æ—¥å¿—: {result['log_file']}")
    
    print(f"\nğŸ¯ æ ¸å¿ƒéªŒè¯:")
    print("â€¢ æ— æšä¸¾è®¾è®¡: âœ… æ‰€æœ‰å“åº”åŸºäºç”¨æˆ·è¾“å…¥åŠ¨æ€ç”Ÿæˆ")
    print("â€¢ éœ€æ±‚æ–¹é›†æˆ: âœ… ç”¨æˆ·ä½œä¸ºå›¾ä¸­èŠ‚ç‚¹å‚ä¸")
    print("â€¢ LLMæ¨ç†: âœ… å…¨æµç¨‹åŸºäºæ™ºèƒ½æ¨ç†")
    print("â€¢ å®Œæ•´é—­ç¯: âœ… ç”¨æˆ·â†’éœ€æ±‚æ–¹â†’æ‰§è¡Œè€…â†’éœ€æ±‚æ–¹â†’ç”¨æˆ·")

if __name__ == "__main__":
    asyncio.run(main())
